{"path":"P7 - Maths/_handouts/Linalg 6.pdf","text":"1 IB Paper 7: Linear Algebra Handout 6 7. Least Squares Solution of Ax = b and QR factorisation Suppose we have carried out an experiment, in which the parameter b has been measured at different times t, b = 0.25 at t = −1 b = 1.0 at t = 0 b = 1.25 at t = 1 b = 3.50 at t = 2 and that we are seeking to fit a relationship to the data:- DtCb += or for a quadratic fit 2b C Dt Et=+ + where the constants C, D and E are to be found. In matrix form the linear case is general case general quadratic case 1 1 0.25 10 1 1 1 1.25 1 2 3.5 C D −⎡⎤ ⎡ ⎤ ⎢⎥ ⎢ ⎥ ⎡⎤⎢⎥ ⎢ ⎥=⎢⎥⎢⎥ ⎢ ⎥⎣⎦ ⎢⎥ ⎢ ⎥ ⎣⎦ ⎣ ⎦ 1 2 1 2 1 1 ... .. . 1 .. . m m b bC D t b t t ⎡⎤ ⎡ ⎤ ⎢⎥ ⎢ ⎥ ⎡⎤⎢⎥ ⎢ ⎥=⎢⎥⎢⎥ ⎢ ⎥⎣⎦ ⎢⎥ ⎢ ⎥ ⎣⎦ ⎣ ⎦ 2 11 1 2 222 2 1 1 ...... ... ... 1 m mm tt b C btt D E btt ⎡⎤ ⎡ ⎤⎢⎥⎡⎤ ⎢ ⎥⎢⎥⎢⎥ ⎢ ⎥=⎢⎥⎢⎥ ⎢ ⎥⎢⎥⎢⎥ ⎢ ⎥⎣⎦⎢⎥ ⎣ ⎦⎣⎦ A x = b These equations are obviously inconsistent and there is no way C and D (and E) can be found to solve all of them. We need, instead, to find x which represents in some sense \"the best fit\". as close as possible toAx b Now the number of columns in A is the number of arbitrary constants in the function used for the fit, and the number of rows is the number of data points. For least squares problems, then, the m × n matrix A usually has the following properties:- (i) m > n (often m >> n) (ii) the columns of A are independent. (rank of A is n.) We will assume that (i) and (ii) hold. b1 = 0.25 b2= 1 b3 = 1.25 b4 = 3.5 0 1 −1 2 1 2 3 2 The least squares solution for x (= x ) minimises () () 2 −= − −Ax b Ax b . Ax b and this can be multiplied out and then partial differentiation used to find the minimum. A, perhaps more intuitive way, is based on geometrical reasoning. This starts by noting that Ax = 11 2 2 3 3xx x++ +a a a ... lies in column space, so the nearest point will be at the end of the “perpendicular” dropped from b onto column space. We saw in sections 5.3 that column space and the left nullspace of A were orthogonal complements. i.e. that for any vector col left=+bb b where 0col left =b. b So we need to get rid of leftb and just concentrate on colb . We can do this by multiplying the original problem by tA . = ttAA x A b col left=+ ttAb Ab The solution of this is x . For the specific example described above bAxAA tt = ⇒ 1 1 0.25 11 1 1 1 0 11 1 1 1 1 0 1 2 1 1 1 0 1 2 1.25 1 2 3.5 C D −⎡ ⎤⎡ ⎤ ⎢ ⎥⎢ ⎥ ⎡⎤ ⎡ ⎤ ⎡⎤⎢ ⎥⎢ ⎥=⎢⎥ ⎢ ⎥ ⎢⎥⎢ ⎥⎢ ⎥−−⎣⎦ ⎣ ⎦ ⎣⎦ ⎢ ⎥⎢ ⎥ ⎣ ⎦⎣ ⎦ i.e. 42 6 26 8 C D ⎡⎤ ⎡ ⎤ ⎡ ⎤ =⎢⎥ ⎢ ⎥ ⎢ ⎥ ⎣⎦ ⎣ ⎦ ⎣ ⎦ ⇒ C = 1 and D = 1 Best fit is 1bt=+ Were we lucky that AA t turned out to be invertible/non-singular ? Column space Left Null Space Rm orthogonal 3 7.1 Useful properties of the matrix tAA AA t is a benign matrix as the following properties show. We can, therefore, always tackle least squares problems using bAxAA tt = . 1) Since A is m × n, At is n × m and AA t is n × n . i.e. AA t is a square matrix. The equation bAxAA tt = thus represents n equations for n unknowns. Good start, but does ( ) 1t − AA exist ? 2) A has independent columns, i.e. the rank of A = r = n and we saw earlier that this is also the number of independent rows of A. The dimension of row space is thus also n. This means that the dimension of the null-space of A is n − r = 0, so that A x = 0 has only the solution x = 0 3) t =AA x 0 ⇒ tt =xA Ax 0 ⇒ () t =Ax Ax 0 i.e. 2 =Ax 0 ⇒ =Ax 0 ⇒ =x0 i.e. the dimension of the null-space of AA t is also 0. If follows that the rank of AA t (= the dimension of column space of AA t ) is also n. i.e. The column space of AA t is the whole of Rn (which is another way of saying that the matrix has an inverse). In summary, the least squares solution to an inconsistent system Ax = b of m equations in n unknowns satisfies bAxAA tt = Assuming that the columns of A are independent, AA t is invertible and ( ) bAAAx t1t − = We note in passing, that the projection of b onto the column space of A is therefore ( ) 1tt col − ==bA x A A A A b The expressions for x and colb are a bit of a handful ⇒we need another method. Note that ( ) 1t − AA is most certainly not () 1t1 −− AA A is not square it doesn’t have an inverse. 4 7.2 Orthogonal basis of Column Space - the Gram-Schmidt process. The equation bAxAA tt = is fine, but we have to do quite a lot of work to follow through with this method when m and n are large (forming AA t alone takes () 212 nm − operations, before we even set about solving the equation). The reason for multiplying by tA is so that we can remove the part of b that is not in the column space of A. Another way of doing this is to project b directly onto column space. colb = λ1 a1 + λ2 a2 + ... Finding the λ’s, however, is a major exercise. Because the a’s are not orthogonal, dotting with a1, etc. doesn’t help, a1 . b = λ1 a1 . a1 + λ2 a1 . a2 + ... If we do this with all of the a’s we will have a matrix to invert for the λ’s. Think how much easier this would be if column space was aligned with our co-ordinate directions, so that, i, j, k, l ... lay in column space (and the other co-ordinate base vectors m, n, ... lay in left- null space). We would then write 12 3bb i b j b k=+ + + ... and simply strip off the ones outside column space. Moreover, if we didn’t have them already, we would generate the coefficients by 1bi b= . , 2bj b= . etc. The Gram-Schmidt procedure is a way of generating a set of mutually orthogonal unit vectors (orthogonal + unit = orthonormal) from an arbitrary set. Armed with these, taking projections is much easier. colb = α1 q1 + α2 q2 + ... And to find the α’s, we simply employ q1 . b = α1 , etc We start with a1 , a2 , ... , an , the columns of A and derive the q’s as follows:- 5 1) Turn the first one into a unit vector 1 1 1 a a q = q1 is in column space Remember, the notation means the \"length\" of an n-dimensional vector 22 2 2 1 nd...dd +++=d , generalised in the obvious fashion. 2) Take a2 and form q2 by first subtracting off the bit that's parallel to a1 and then normalising ( )12122 qaqaa .~ −= 2 2 2 a a q ~ ~ = Check q2 is in column space because it is a linear combination of a2 and q1 3) Repeat this process for the other a's. ( )( )23213133 qaqqaqaa ..~ −−= 3 3 3 a a q ~ ~ = etc. Check () ( )13 13 1 3 1 1 2 3 1 2 1 3. . .. .. .=− − = ⇒ =qa qa q a q q q a q q 0 q q 0% () ( )23 23 1 3 2 1 2 3 2 2 2 3.. . . . . .=− − = ⇒ =qa qa q a q q q a q q 0 q q 0% Note that, since the columns of A are independent, we never have k ~a = 0 . So this Gram-Schmidt orthogonalisation process, will always furnish an orthonormal set of n vectors. Any vector in the column space can, by definition, be written as a linear combination of the a's and so as a linear combination of the q's. i.e. q1, q2, ... , qn is an (orthonormal) basis for the column space of A. a1 a2 ()1 2 1 2 1 211 12 .. . . . =− = ⇒= qa q a q a q q 0 qq 0 % 6 Example Perform Gram-Schmidt orthogonalisation on 1 1 0 1 ⎡⎤ ⎢⎥= ⎢⎥ ⎢⎥⎣⎦ a 2 1 0 0 ⎡⎤ ⎢⎥= ⎢⎥ ⎢⎥⎣⎦ a 3 2 1 0 ⎡⎤ ⎢⎥= ⎢⎥ ⎢⎥⎣⎦ a 1) 1 1 1 a a q = = 1 2 0 1 2 ⎡⎤ ⎢⎥ ⎢⎥ ⎢⎥ ⎢⎥ ⎢⎥ ⎢⎥⎣⎦ i.e. 1 2 20 11 2 1 0 ⎡ ⎤ ⎢ ⎥ ⎢ ⎥ = ⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥⎣ ⎦ ⎡⎤ ⎢⎥ ⎢⎥ ⎢⎥⎣⎦ 2) Subtract off the bit of a2 that is parallel to q1 and then create a unit vector ()12122 qaqaa .~ −= = 1 1 1 2 2 1 00 0 2 01 1 22 ⎡⎤ ⎡ ⎤ ⎢⎥ ⎢ ⎥⎡⎤ ⎢⎥ ⎢ ⎥⎢⎥−=⎢⎥ ⎢ ⎥⎢⎥ ⎢⎥ ⎢ ⎥⎢⎥⎣⎦ ⎢⎥ −⎢ ⎥ ⎣ ⎦⎢⎥⎣⎦ 1 2 20 1 2 ⎡ ⎤ ⎢ ⎥ ⎢ ⎥ = ⎢ ⎥ ⎢ ⎥ ⎢ ⎥− ⎢ ⎥⎣ ⎦ 2q = 1 2 0 1 2 ⎡⎤ ⎢⎥ ⎢⎥ ⎢⎥ ⎢⎥ ⎢⎥− ⎢⎥⎣⎦ 3) Subtract off the bits of a3 that are parallel to q1 and q2 and then create a unit vector () ( )23213133 qaqqaqaa ..~ −−= = 11 2 22 12 0 2 0 01 1 2 0 1 0 2 ⎡⎤ ⎡ ⎤ ⎢⎥ ⎢ ⎥⎡⎤ ⎢⎥ ⎢ ⎥⎢⎥−−⎢⎥ ⎢ ⎥⎢⎥ ⎢⎥ ⎢ ⎥⎢⎥⎣⎦ ⎢⎥ ⎢ ⎥− ⎡⎤ ⎢⎥= ⎢⎥ ⎢⎥⎣ ⎢⎥ ⎢ ⎥⎣⎦ ⎦ ⎣⎦ In preparation for what is coming next, let us rewrite this as a relationship between the a’s and the q’s in the form 1 1 2 02 0 11 2 ⎡⎤ ⎢⎥⎡⎤ ⎢⎥⎢⎥= ⎢⎥⎢⎥ ⎢⎥⎢⎥⎣⎦ ⎢⎥ ⎢⎥⎣⎦ 11 1 22 11 00 0 22 01 1 22 ⎡ ⎤⎡ ⎤ ⎢ ⎥⎢ ⎥⎡⎤ ⎢ ⎥⎢ ⎥⎢⎥=+⎢ ⎥⎢ ⎥⎢⎥ ⎢ ⎥⎢ ⎥⎢⎥⎣⎦ ⎢ ⎥⎢ ⎥− ⎢ ⎥⎢ ⎥⎣ ⎦⎣ ⎦ 11 2022 12 0 2 0 1 01 1 0 22 ⎡⎤ ⎡ ⎤ ⎢⎥ ⎢ ⎥⎡ ⎤⎡ ⎤ ⎢⎥ ⎢ ⎥⎢ ⎥⎢ ⎥=+ +⎢⎥ ⎢ ⎥⎢ ⎥⎢ ⎥ ⎢⎥ ⎢ ⎥⎢ ⎥⎢ ⎥⎣ ⎦⎣ ⎦⎢⎥ ⎢ ⎥− ⎢⎥ ⎢ ⎥⎣⎦ ⎣ ⎦ 7 7.3 QR factorisation of A If we assemble the three vectors a1, a2, a3 from the previous section as the columns of a matrix A, and vectors q1, q2, q3 as those of a matrix Q, then we have A = ⎥ ⎥ ⎥ ⎦ ⎤ ⎢ ⎢ ⎢ ⎣ ⎡ 001 100 211 and 11 0 22 00 1 11 0 22 ⎡ ⎤ ⎢ ⎥ ⎢ ⎥ = ⎢ ⎥ ⎢ ⎥ ⎢ ⎥− ⎢ ⎥⎣ ⎦ Q Then ⎥ ⎥ ⎥ ⎦ ⎤ ⎢ ⎢ ⎢ ⎣ ⎡ 001 100 211 = 1 22 2 1 02 2 0 11 0 22 00 1 11 0 2 0 2 1 ⎡ ⎤ ⎡⎤⎢ ⎥ ⎢⎥⎢ ⎥ ⎢⎥⎢ ⎥ ⎢⎥⎢ ⎥ ⎢⎥⎢ ⎥ ⎢⎥− ⎢ ⎥ ⎢⎥⎣⎦⎢ ⎥⎣ ⎦ i.e. We have constructed another matrix factorisation A = Q R The matrix Q has mutually orthogonal unit vectors and the matrix R is upper triangular. Before writing down the general form of this factorisation (we have done it for a 3 × 3 one), we can tidy up the relationship between the a's and the q's. You can see that a3 for example satisfies () ( ) 32321313 aqaqqaqa ~.. ++= () ( ) 33232131 qaqaqqaq ~.. ++= Taking the dot product with q3 gives a neater formula for 3a~ 33 3. =qa a% so that () ( )( )3332321313 qaqqaqqaqa ... ++= The general formula is clear ()1111 qaqa .= () ( )2221212 qaqqaqa .. += () ( ) ( )3332321313 qaqqaqqaqa ... ++= etc. 8 Writing the Gram-Schmidt process as a relationship between matrices (see Section 2.6) :- A = ⎥ ⎥ ⎥ ⎦ ⎤ ⎢ ⎢ ⎢ ⎣ ⎡ ↓↓↓ ↑↑↑ ... ... naaa 21 and Q = ⎥ ⎥ ⎥ ⎦ ⎤ ⎢ ⎢ ⎢ ⎣ ⎡ ↓↓↓ ↑↑↑ ... ... nqqq 21 Then A = ⎥ ⎥ ⎥ ⎦ ⎤ ⎢ ⎢ ⎢ ⎣ ⎡ ↓↓↓ ↑↑↑ ... ... naaa 21 = ⎥ ⎥ ⎥ ⎦ ⎤ ⎢ ⎢ ⎢ ⎣ ⎡ ↓↓↓ ↑↑↑ ... ... nqqq 21 ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ ⎤ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ ⎡ nn n n n . ............... ..... ...... ....... aq aqaq aqaqaq aqaqaqaq 0000 00 0 233 23222 1312111 i.e. A = Q R For the general case A is m × n, Q is the same shape as A m × n, R is n × n. Works for any A, provided rank(A) = n. The columns of Q are mutually orthogonal vectors which span the column space of A. The matrix R is square, upper triangular with non-zero elements down the diagonal. It therefore has rank n and is invertible. See section 4.2 where we discussed this issue for L. 7.4 The Matrix Q We met square matrices like Q in Part IA Maths and studied all of their properties. We have to be careful here, because these matrices are in general rectangular with m > n. It is still true that IQQ = t since QQ t = ⎥ ⎥ ⎥ ⎦ ⎤ ⎢ ⎢ ⎢ ⎣ ⎡ ↓↓↓ ↑↑↑ ⎥ ⎥ ⎥ ⎥ ⎦ ⎤ ⎢ ⎢ ⎢ ⎢ ⎣ ⎡ →← →← →← ... ... ......... n n qqq q q q 21 2 1 = ⎥ ⎥ ⎥ ⎥ ⎦ ⎤ ⎢ ⎢ ⎢ ⎢ ⎣ ⎡ nnnn n n ...... ............ ...... ...... qqqqqq qqqqqq qqqqqq 21 22212 12111 and the q's are orthogonal unit vectors. But this does not imply that Q−1 = Qt . These matrices are not square (in general) Q does not have an inverse (in general) Note also that IQQ ≠ t unless Q is square. 9 7.5 Simplification of the Least Squares solution to Ax = b This is now much less effort using the QR decomposition. Given the set of equations Ax = b, where A is an m × n matrix whose columns are independent (m ≥ n and rank of A = n) then the least squares solution satisfies bAxAA tt = ⇒ () () tt =QR QR x QR b ⇒ tt tt=RQ QR x R Q b But IQQ = t , so that bQRxRR ttt = Further, the square matrix R is invertible, which means that so is Rt . It follows that bQxR t= The right hand side is simply a matrix multiplying a vector and the solution for x is found by back- substitution (R is an upper triangular matrix). Example Find the least squares solution for the problem at the beginning of section 7 11 0.25 10 1 11 1.25 12 3.5 C D −⎡⎤ ⎡ ⎤ ⎢⎥ ⎢ ⎥ ⎡⎤⎢⎥ ⎢ ⎥=⎢⎥⎢⎥ ⎢ ⎥⎣⎦ ⎢⎥ ⎢ ⎥ ⎣⎦ ⎣ ⎦ Step 1: QR decomposition of A 1) 1 12 12 2 12 12 ⎡⎤ ⎢⎥ ⎢⎥= ⎢⎥ ⎢⎥ ⎣⎦ a 1 2= q 1 12 12 12 12 ⎡ ⎤ ⎢ ⎥ ⎢ ⎥= ⎢ ⎥ ⎢ ⎥ ⎣ ⎦ q 2) ()12122 qaqaa .~ −= = 2 11 2 3 2 01 2 1 2 15 11 2 1 2 21 2 3 2 −−⎡⎤ ⎡ ⎤ ⎡ ⎤ ⎢⎥ ⎢ ⎥ ⎢ ⎥−⎢⎥ ⎢ ⎥ ⎢ ⎥−= = ⎢⎥ ⎢ ⎥ ⎢ ⎥ ⎢⎥ ⎢ ⎥ ⎢ ⎥ ⎣⎦ ⎣ ⎦ ⎣ ⎦ q 2q = 3 25 1 25 1 25 3 25 ⎡ ⎤ −⎢ ⎥ ⎢ ⎥ ⎢ ⎥ −⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎣ ⎦ 10 3) 13 2 25 11 11 10 2 25 11 1 1 212 25 13 2 21 5 25 0 ⎡⎤ −⎢⎥ ⎢⎥ −⎡⎤ ⎢⎥ −⎢⎥ ⎢⎥ ⎢⎥ ⎢⎥= ⎢⎥ ⎢⎥ ⎢⎥ ⎢⎥ ⎣⎦ ⎢⎥ ⎢⎥ ⎢⎥ ⎣⎦ ⎡⎤ ⎢⎥ ⎣⎦ A = Q R Step 2 Solve bQxR t= by back-substitution. 0.2511 1 1 3122 2 2 31 1 3 1.25 5 25 25 25 25 3. 21 5 05 C D ⎡⎤⎡⎤ ⎢⎥⎢⎥ ⎡ ⎤⎡⎤ ⎢⎥⎢⎥== ⎢ ⎥⎢⎥ ⎢⎥⎢⎥⎣⎦ ⎣ ⎦−− ⎢⎥⎢⎥⎣⎦⎣ ⎡ ⎢ ⎣ ⎦ ⎤ ⎥ ⎦ C = 1, D = 1 7.6 Operation Count and Robustness of QR QR factorisation is more costly than LU decomposition (the cost is primarily in the Gram-Schmidt process). LU is thus preferable for solving sets of consistent equations. For inconsistent equations (i.e. a genuine least-squares problem), QR is more cost effective than solving bAxAA tt = by LU decomposition. In addition, the matrix AA t is often numerically poorly conditioned, so it is a not a good idea to go via bAxAA tt = . The Gram-Schmidt process can, for large n, become ill-conditioned (you are finding the q's by a process of subtracting a large number of things and then normalising to unity). There are other ways of finding a Q, but these are beyond the scope of this course. 7.7 Projection onto Column Space In section 7.1, we showed that colb , the projection of b onto the column space of A, satisfies () 1tt col − =bA A A A b i.e. Pb = colb where the projection matrix P is given by ( ) t1t AAAAP − = This rather complicated expression for P, was the reason that we developed the QR method. 11 There are a number of other applications were it is useful to be able to easily project onto column space, and the QR decomposition should give us a much simpler expression for this projection. If we have performed the decomposition A = QR, then () 1tt tt− = QR R Q QR R QP () 1tt t− = QR R R R Q () 11t t t−−= QRR R R Q i.e. P t= QQ This is as expected (!) since () ( ) ( )11 2 2. . ... .col n n=+ + +bq b q q b q q b q colb = ⎥ ⎥ ⎥ ⎥ ⎦ ⎤ ⎢ ⎢ ⎢ ⎢ ⎣ ⎡ ⎥ ⎥ ⎥ ⎦ ⎤ ⎢ ⎢ ⎢ ⎣ ⎡ ↓↓↓ ↑↑↑ bq bq bq qqq . ... . . ... ... n n 2 1 21 = ⎥ ⎥ ⎥ ⎦ ⎤ ⎢ ⎢ ⎢ ⎣ ⎡ ↓ ↑ ⎥ ⎥ ⎥ ⎥ ⎦ ⎤ ⎢ ⎢ ⎢ ⎢ ⎣ ⎡ →← →← →← ⎥ ⎥ ⎥ ⎦ ⎤ ⎢ ⎢ ⎢ ⎣ ⎡ ↓↓↓ ↑↑↑ b q q q qqq n n ......... ... ... 2 1 21 = bQQ t Key Points from Lecture QR Decomposition A = QR, where Q us the same shape as A and the columns of Q are orthonormal, and R is square, upper-triangular and invertible. When m = n and so all matrices are square, Q is an orthogonal matrix. Least squares solution of Ax = b using QR Solve bQxR t= by back-substitution. You can now do Examples Paper 2 Q1-3","libVersion":"0.2.4","langs":""}