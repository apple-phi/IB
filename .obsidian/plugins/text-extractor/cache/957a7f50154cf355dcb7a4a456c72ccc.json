{"path":"P7 - Maths/_handouts/5. Decision, Estimation and Hypothesis Testing.pdf","text":"2P7: Probability & Statistics Decision, Estimation and Hypothesis Testing Thierry Savin Lent 2024 th e ro y al ﬂ ush , t h e b e s t p os s i b l e h a n d i n p ok e r , h a s a p r ob a b i l i t y 0. 000154% Introduction Course’s contents 1. Probability Fundamentals 2. Discrete Probability Distributions 3. Continuous Random Variables 4. Manipulating and Combining Distributions 5. Decision, Estimation and Hypothesis Testing 1/15 Introduction This lecture’s contents Introduction Decision and estimation theory Hypothesis testing Course survey 2/15 Introduction Decision, Estimation and Hypothesis Testing In the last lectures, we have seen: I that discrete random variables are described by their probability mass function I that continuous random variables are described by their probability density function I how to manipulate random variables and their distributions. I the central limit theorem. In this lecture, we will I learn how to make decisions or estimates on random variables using their measurements I explain how to assess the statistical signiﬁcance of an experiment using hypothesis testing. Unfortunately, we will only scratch the surface of these vast subjects. . . 3/15 Decision and estimation theory Problem statement & deﬁnitions I Suppose that the PDF (or PMF) of a random variable X is a function f X (x; ✓)(or P X (x; ✓)) that depends on a parameter ✓. I We wish to ﬁnd the value of ✓ from observations of X: • If ✓ can adopt a continuous set of values, we want to best estimate its value given the observations. • If ✓ can only take a value from a discrete set, we want to decide which value is the correct one given the observations. I We deﬁne an observation as a measurement of X.We usually have n observations x =[x1,. .., xn]T. I We deﬁne the sample as the set of random variables underlying the observations X =[X1,. .., Xn]T. Usually, these are independent and identically distributed (i.i.d.) with f X i (x; ✓)= f X (x; ✓)(or P X i (x; ✓)= P X (x; ✓)) for all i 2 {1,. .., n}. I The estimate (or decision)for ✓ is a function of the observations, ˆ✓(x). I The estimator (or decision rule)is thecorresponding random variable ˆ⇥ = ˆ✓(X). The goal is to ﬁnd an expression for ˆ✓. 4/15 Decision and estimation theory Bayesian approach In Bayesian statistics: I The unknown parameter ✓ is viewed as the value of a random variable ⇥; I The distribution of the sample is then interpreted as the conditional distribution f X | ⇥ (x|✓)(or P X | ⇥ (x|✓)); I The prior information is used to assign somehow aPDF f ⇥ (✓) (or PMF P ⇥ (✓)) to the random variable ⇥. I The problem of estimating (or deciding) the unknown parameter ✓ is thus changed to the problem of predicting the value ✓ of the random variable ⇥. I We deﬁne: • the prior function f ⇥ (✓)or P ⇥ (✓)(priorto the measurements) • the likelihood function f X | ⇥ (x|✓)or P X | ⇥ (x|✓) • the posterior function f ⇥ | X (✓|x)or P ⇥ | X (✓|x)(after the measurements) 5/15 Decision and estimation theory Common estimators I Maximum likelihood estimator (ML) ˆ✓ML(x)= arg max ✓ f X | ⇥ (x|✓) I Maximum a posteriori estimator (MAP) ˆ✓MAP(x)= arg max ✓ f ⇥ | X (✓|x) I Minimum mean squared error (MMSE) ˆ✓MMSE(x)= E[⇥|X = x]= Z ✓ f ⇥ | X (✓|x)d✓ minimises the mean squared error E[(✓ \u0000 ⇥)2|X = x]. Note that I The posterior function is obtained from Bayes’ rule: f ⇥ | X (✓|x)= f X | ⇥ (x|✓)f ⇥ (✓) f X (x) I If f ⇥ (✓) = constant (i.e. ⇥ uniformly distributed), then ˆ✓ML = ˆ✓MAP. only this needs to be maximised for ˆ✓MAP doesn’t depend on ✓ 6/15 Decision and estimation theory Examples We draw n observations {x1,. .., xn} of the random variable X ⇠ N (✓, 1) from an i.i.d. sample {X1,. .., Xn}.What is the ML estimator of ✓? The likelihood function is1: f X | ⇥ (x|✓)= f X 1 | ⇥ (x1|✓)⇥f X 2 | ⇥ (x2|✓)⇥· · ·⇥f X n | ⇥ (xn|✓)= e\u0000 1 2 Pn i=1(xi \u0000✓)2 (2⇡)n/2 Maximising f X | ⇥ (x|✓)isminimising Pn i=1(xi \u0000 ✓)2: d d✓ nX i=1 (xi \u0000 ✓)2 = \u00002 nX i=1 (xi \u0000 ✓)= 2n✓ \u0000 2 nX i=1 xi =0 is solved for ˆ✓ML(x)= 1 n nX i=1 xi = ¯x,the samplemean. 1 Why can we write f X 1 ... X n | ⇥ = f X 1 | ⇥ ⇥f X 2 | ⇥ ⇥...⇥f X n | ⇥ ? We haven’t seen conditional independence,and it may look a bit peculiar. First note, that f X 1 ... X n ⇥ = f X 2 ... X n | X 1 ⇥ ⇥f X 1 ⇥ .Since X2,..., Xn are conditionally independent of X1, f X 2 ... X n | X 1 ⇥ = f X 2 ... X n | ⇥ = f X 2 ... X n ⇥ / f ⇥ and f X 1 ... X n ⇥ = f X 2 ... X n ⇥ ⇥f X 1 ⇥ / f ⇥ = f X 2 ... X n ⇥ ⇥f X 1 | ⇥ . So we’ve proved f X 1 ... X n ⇥ = f X 1 | ⇥ ⇥f X 2 ... X n ⇥ .The same way, f X 2 ... X n ⇥ = f X 2 | ⇥ ⇥f X 3 ... X n ⇥ ,and so on. Weget f X 1 ... X n ⇥ = f X 1 | ⇥ ⇥f X 2 | ⇥ ⇥...⇥f X n\u0000 1 | ⇥ ⇥f X n ⇥ and dividing by f ⇥ on both sides gives the result. 7/15 Decision and estimation theory Examples We draw n observations {x1,. .., xn} of the random variable X ⇠ N (✓, 1) from an i.i.d. sample {X1,. .., Xn},with the belief that ⇥ ⇠ N (#, \u00002). What is the MAP estimator of ✓? The posterior function is: f ⇥ | X (✓|x) / f X | ⇥ (x|✓)f ⇥ (✓) / e\u0000 1 2 ⇥\u0000 ✓\u0000# \u0000 \u00002+Pn i=1(xi \u0000✓)2⇤ (2⇡)n/2 Maximising f ⇥ | X (✓|x)isminimising \u0000 ✓\u0000# \u0000 \u00002 + Pn i=1(xi \u0000 ✓)2: d d✓ ✓ ✓ \u0000 # \u0000 ◆2 + nX i=1 (xi \u0000 ✓)2\u0000 =2 ✓ \u0000 # \u00002 +2n(✓ \u0000 ¯x)=0 is solved for ˆ✓MAP(x)= # + n\u00002¯x 1+ n\u00002 . I We verify ˆ✓MAP(x) \u0000!1 \u0000\u0000\u0000\u0000\u0000! ˆ✓ML(x)= ¯x as the prior is “ﬂattened” (hence uniform) when \u0000 !1. I Here the posterior is Gaussian, for which maximum and mean coincide, and ˆ✓MMSE(x)= ˆ✓MAP(x). 8/15 Decision and Estimation theory Examples We draw n observations {x1,. .., xn} of the random variable X ⇠ U(0, ✓) from an i.i.d. sample {X1,. .., Xn}.That is, the common PDF is f X i (xi )= ⇢ ✓\u00001 if 0  xi  ✓, 0 otherwise; ,for all i =1 .. . n. What is the ML estimator of ✓? The likelihood function in this case is given by f X | ⇥ (x|✓)= 1 ✓n 0  xi  ✓ for i =1 .. . n = 1 ✓n 0  max{x1,. .., xn}  ✓ It is maximised by the minimum value of ✓ and since ✓ \u0000 max{x1,. .., xn}, we get ˆ✓ML(x)= max{x1,. .., xn} . 9/15 Hypothesis testing Principle Hypothesis testing is designed to tell us if the observations indicate that something “unusual” or “interesting” happened with an experiment. I We are concerned with the statistical signiﬁcance of an e↵ect in our data, against the hypothesis that no such e↵ect is present. The latter hypothesis is called the null hypothesis and is written H0. I In other words, a result has statistical signiﬁcance when it is very unlikely to have occurred given the null hypothesis. I More precisely: • We deﬁne the p-value of a result as the probability p of obtaining a result at least as extreme, given H0 is true. • Choose a signiﬁcance level, denoted by ↵ (typically 1% or 5%). If p  ↵,the result is unlikely to happenunder H0:we reject H0 and the result is said to be statistically signiﬁcant. This is probabilistic reductio ad absurdum. 10/15 Hypothesis testing The null hypothesis H0 -Examples The null hypothesis H0 is typically a statement of “no e↵ect”or “no di↵erence”. Question Null hypothesis H0 Are boys taller than girls at age eight? “they are the same average height” Do teens use restaurant locator apps more than adults? “they use these apps the same amount” Does eating an apple a day reduce visits to the doctor? “apples do not reduce doctor visits” Are small states more densely populated than large states? “small states have the same population density as large states” Does the size of a state a↵ect population density? “all states have the same population density” Do large dogs prefer large food kibbles? “large dogs have no preference for kibble size” Do cats prefer ﬁsh or milk? “cats have no preference; they like them the same” 11/15 Hypothesis testing Example Can people, in a blind test, taste the di↵erence between single malt and blended whisky? We get 10 people to blind test, each given a randomly selected drink: I We observe that 7 people correctly identify their drink, and 3 respond incorrectly. What do we conclude? Null hypothesis H0: “people can’t taste the di↵erence” (i.e. they give a random response). Under H0,the number X of correct people is binomial: X|H0 ⇠ B(10, 1/2) . Assuming the null hypothesis is true, what is the probability of the observed outcome, or something more extreme? 10X k=7 10Ck ⇣ 1 2 ⌘k ⇣1 \u0000 1 2 ⌘10\u0000k ⇡ 0.172 = 17.2% So, under the hypothesis that people respond randomly, this outcome or something more extreme would happen in about 17% of cases. This doesn’t constitute strong evidence against the hypothesis. 12/15 Hypothesis testing Example I We repeat the experiment with 100 people, and get 70 correct, 30 incorrect. What do we now conclude? Under H0, the probability of the observed outcome, or something more extreme, is now P100 k=70 100Ck \u0000 1 2 \u0000k \u00001 \u0000 1 2 \u0000100\u0000k . To calculate this, remember that for n independent Bernoulli trials {Xi ⇠ Ber(p)}i=1...n we have Pn i=1 Xi ⇠ B(n, p). Invoking the central limit theorem, B(n, p) n\u00001 ⇡ N \u0000 np, np(1 \u0000 p)\u0000 and B(100, 1/2) ⇡ N \u000050, 25\u0000.Hence the p-value: 30 40 50 60 70 0.03 0.06 B (100 , 1 / 2 ) N (50 , 25) 100X k=70 100Ck ⇣ 1 2 ⌘k ⇣1 \u0000 1 2 ⌘100\u0000k ⇡ 1 \u0000 \u0000⇣ 70 \u0000 50 p25 ⌘ =1 \u0000 \u0000(4) < 10\u00004 Under H0, this outcome is exceedingly unlikely and we can thus reject H0 (i.e. people can actually taste the di↵erence). 13/15 Hypothesis testing One-sided vs two-sided We want to establish whether a coin is fair. Out of 10 tosses, we get 7 heads and 3 tails. What is the evidence against the null hypothesis that the coin is fair? Under H0, the probability of the observed outcome, or something more extreme, is now: 10X k=7 10Ck ⇣ 1 2 ⌘k ⇣1 \u0000 1 2 ⌘10\u0000k + 3X k=0 10Ck ⇣ 1 2 ⌘k ⇣1 \u0000 1 2 ⌘10\u0000k = 34.4% where we account both tails of the distribution. Observing 3 heads (or less) is also as extreme (or more) as the observed outcome. I Note how the coin example is di↵erent from the whisky tasting example. The tasting example is asymmetric:we test whether people are better than chance at distinguishing two types of whisky, not whether their ability is di↵erent from chance. I Figuring out whether one-sided or two-sided tests are appropriate may require some attention. . . 14/15 Hypothesis testing Remarks about the p-value I The p-value is the probability of observing an e↵ect “at least as extreme as” the one in your sample data, under the assumption that no such e↵ect is present in the population your observations are drawn from. Why “a result at least as extreme”? Using only the particular observed outcome may lead to small probabilities. Consider the whisky blind test with 500 people being correct out of 1000. Then P[500|H0]= 1000C500 21000 =0.025 < 5%, and H0 would be rejected. . . I Hypothesis testing is used a lot. One of the most common misuses of the p-value is to assign it to the probability of the null hypothesis. This is wrong,as weknow: P[results|H0] 6= P[H0|results] 6= P[H0] This mistake has led to some dramatic miscarriages of justice. . . I The alternative, sometimes written H1 = H { 0 does not carry a lot of information. 15/15 You can attempt all problems of Examples Paper 6","libVersion":"0.2.4","langs":""}