{"path":"P8 - Information/_handouts/learning-part-2-handout.pdf","text":"Deep Learning P ar t 2: Con v olutional Neural Netw orks Dr . Richard E. T ur ner ( ret26@cam.ac.uk ) Engineer ing T r ipos P ar t IB P aper 8: Inf or mation Engineer ing Big picture • Goal : produce automatic systems that solv e image recognition prob lems – detect and classify objects into categor ies , independently of pose , sca le , lighting and w eather conditions , occlusion and clutter • initial computer vision appr oac h : hand-cr afted f eatures (not rob ust) • deep learning appr oac h : lear n suitab le systems directly from labelled images road paveme nt building car 1 cyclist 1 cyclist 2 car 2 t rees t ra f f ic light s pe dest r ian 1 t ra f f ic light s int ent lane cyclist 3 signals green Appr oac h 1. use deep neural netw orks that lear n representations of images with m ultiple la y ers of abstr action • the st r ucture of visu al scenes and hier archical organisat ion of biologica l vision are motiv ations f or this approach 2. design bespoke neural netw orks f or ima g es so that the y can handle input images with millions of pix els • m ulti-la y er perceptrons (MLPs) cannot handle such high dimensional input s • le v er age proper ties of images to design con v olutional neural netw orks 3. impr o ve the training algorithm so that it can scale to man y images • optimisation via g r adient descent is slo w f or large n umbers of data points • de v elop stoc hastic gradient ascent to impro v e scalability Wh y use hierar c hical m ulti-la y ered models i.e . deep learning? t re es ba r k, le ave s, et c. or i en t e d ed ge s f or est i m ag e ob je ct ob je ct pa r t s pr i m it ive f ea t u r es in pu t im a ge Ar gu m en t 1: vi sua l scen es ar e h ie r ach ica ll y o r ga ni sed Wh y use hierar c hical m ulti-la y ered models i.e . deep learning? t re es ba r k, le ave s, et c. or i en t e d ed ge s f or est i m ag e ob je ct ob je ct pa r t s pr i m it ive f ea t u r es in pu t im a ge Ar gu m en t 2: b io lo gi cal vi sio n is hi er a chi cal ly or g an ise d ph ot o- r e cep t o r s r et in a V1: si m pl e an d com p le x ce ll s V4: d if f er e nt t ext ur es I nf er ot em p or a l cor t ex Couldn’t we use standar d neural netw orks like MLPs? How many p ar amet ers does t his neural net work have? F or a sma ll 32 by 32 image: Hard to trai n over-f it t ing and local op t ima slow t o converge Hu g e mem o ry fo o tp ri n t limit s com pl exit y of t ra ined net works Co n vo l u ti o n al n ets red u ce th e n u m b er o f p aram eters w i th o u t co mp ro mi si n g n et w o r k c o m p l exi ty number of paramet ers The ke y ideas behind con v olutional neural netw orks • ima g e statistics are translation in v ariant (objects and vie wpoint tr anslates) – b uild this tr anslation in v ar iance into the model (r ather than lear ning it) – tie lots of the w eights together in the netw or k – reduces n umber of par ameters • e xpect learned lo w-le vel f eatures to be local (e .g. edge detector) – b uild this into the model b y allo wing only local connectivity – reduces the n umbers of par ameters fur ther • e xpect high-le vel f eatures learned to be coar ser (c.f . biology) – b uild this into the model b y subsampling more and more up the hier arch y – reduces the n umber of par ameters again Building b loc k of a con v olutional neural netw ork convolut io nal st ag e non-linear st ag e po oling st ag e inp ut image mean or subsa mp le also used e. g. only p ara met ers Full con v olutional neural netw ork convolut ional st age ' norma l' neural net work la y e r 1 la y e r 2 non-lin ear st age non-lin ear st age convolut ional st age non-lin ear st age non-lin ear st age will have dif f erent f ilt ers connect s t o several f eat ure maps Ho w man y parameter s does a con v olutional netw ork ha ve? How many p ar amet ers does t his neural net work have? F or a sma ll 32 by 32 image: T raining: gradient descent Output of con v olutional neur al netw or k inter preted as a c lass label pr obability x ( n ) = x ( z ( n ) ; W )= p ( y ( n ) =1 | z ( n ) , W ) No w x ( n ) is a more comple x function of the inputs z ( n ) and contains lots of par ameters W . Optimise the cr oss entr op y (with regular isation / w eight deca y) G ( W )= N X n =1 ⇣ y ( n ) l og x ( n ) +( 1 \u0000 y ( n ) ) l og ( 1 \u0000 x ( n ) ) ⌘ = N X n =1 l ( y ( n ) ,x ( n ) ) But, will tak e a long time to accum u late objectiv e and der iv ativ es o v er all N tr aining data points . d G ( W ) d W = N X n =1 d l ( y ( n ) ,x ( n ) ) d W W W \u0000 ⌘ ✓ d G ( W ) d W \u0000 ↵ W ◆ T raining: stoc hastic gradient descent Obser v ation : often onl y need to see a small n umber of ima g es bef ore y ou g et a good idea of the gradient direction Idea : at eac h step of gradient descent, randoml y select M  N data points fr om the training dataset { t ( m ) , z ( m ) } M m =1 and estimate the objective / gradient fr om these G ( W )= N X n =1 l ( t ( n ) ,x ( n ) ) ⇡ N M M X m =1 l ( t ( m ) ,x ( m ) ) d G ( W ) d W = N X n =1 d l ( t ( n ) ,x ( n ) ) d W ⇡ N M M X m =1 d l ( t ( m ) ,x ( m ) ) d W T ur n do wn lear ning r ate ⌘ as lear ning prog resses (e .g. using a v alidation dataset). T raining: ad ditional tips and tric ks • bac k-pr opa gation f or training : stochastic g r adient ascent – tak e care to initialise scales of the w eights correctly (high w eights cause the output to satur ate and the g r adients f all to z ero) – using non-satur ating hidden units can mak e the optimisation simpler (e .g. rectiﬁed linear hidden units r ather than sigmoid units) • data-augmentation : alw a ys impro v es perf or mance substantially (include shifted, rotations , mirror ing, cropping, locally distor ted v ersions of the tr aining data, computer g r aphics) • typical n umber s : – V GG-16: 16 con v olutional la y ers , ⇡ 100 channels , 3 ⇥ 3 ﬁlters , 3 la y ers of 1000 units in top neur al netw or k – 100 million par ameters – 1 w eek to tr ain on the imagenet database (100GB , GPUs) Demo CIF AR 10 dataset: 50,000 tr aining images , 10,000 test images http://matajoh.github.io/anndemos/ http://cs.stanford.edu/people/karpathy/ convnetjs/demo/cifar10.html Looking into a con v olutional neural netw ork’s brain t op 9 im a ge p at che s t ha t cau se m axi m al a ct i vat io n in l aye r 2 u ni t r eco nst r uct io n of i m ag e pa t ch es f ro m t ha t un it ( in di cat es asp ect o f pa t ch es wh ich u ni t is sen sit ive t o) Looking into a con v olutional neural netw ork’s brain Looking into a con v olutional neural netw ork’s brain Summar y • con v olutional neur al netw or ks are the go-to model f or computer vision classiﬁcation prob lems – f or m of neur al netw or k with an architecture suited to vision prob le ms (con v olutional str ucture and pooling) – higher le v el la y ers encode more abstract f eatures – higher le v el la y ers sho w more in v ariance to instantiation parameter s (tr anslation, rotation, lighting changes , etc.) • tr ained on a labelled tr aining data set b y optimising the regular ised cross- entrop y using stoc hastic gradient descent • netw or k automatically learns f eature detector s – ﬁrst la y er lear ns edge detectors – subsequent la y ers more comple x (cor ner detectors , b lob detectors etc.) – integ r ates tr aining of the classiﬁer with tr aining of the f eat ure representatio n","libVersion":"0.2.4","langs":""}