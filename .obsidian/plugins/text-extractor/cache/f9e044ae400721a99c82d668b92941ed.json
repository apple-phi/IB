{"path":"P8 - Information/_handouts/summary-lecture-2.pdf","text":"Deep Learning Summary of lecture 2 Dr. Richard E. Turner (ret26@cam.ac.uk) Engineering Tripos Part IB Paper 8: Information Engineering Summary of lecture 2 relative entropy / data fit fitting method 1: maximum likelihood fit Summary of lecture 2 −5 0 5 −5 0 5 over-fitting relative entropy / data fit fitting method 1: maximum likelihood fit Summary of lecture 2 fitting method 2: regularised maximum likelihood \"regulariser\" prevents extreme weights weight decay −5 0 5 −5 0 5 over-fitting relative entropy / data fit fitting method 1: maximum likelihood fit Summary of lecture 2 fitting method 2: regularised maximum likelihood \"regulariser\" prevents extreme weights weight decay −5 0 5 −5 0 5 −5 0 5 −5 0 5 over-fitting well fit relative entropy / data fit fitting method 1: maximum likelihood fit Question 1 What is the probability of the observed labels given the inputs and weights? Observe 3 labelled data points with scalar inputs and I have single neuron: 0 1 3 3 0.9 0.7 0.1 1 2 3 1 1 2 2 0 0 A. B. C. D. E. I don't know! Question 1 What is the probability of the observed labels given the inputs and weights? Observe 3 labelled data points with scalar inputs and I have single neuron: 0 1 3 3 0.9 0.7 0.1 1 2 3 1 1 2 2 0 0 A. B. C. D. E. I don't know! Learning by improving the likelihood of the parameters 1 What is the probability of the observed labels given the inputs and weights? Observe 3 labelled data points with scalar inputs and I have single neuron: 0 1 improving the likelihood will: increase x at y=1 examples decrease x at y=0 examples also known as the likelihood of the parameters 3 3 0.9 0.7 0.1 1 2 3 1 1 2 2 0 0 A. B. C. D. E. I don't know!","libVersion":"0.2.4","langs":""}