{"path":"P8 - Information/_handouts/learning-part-1-handout.pdf","text":"Deep Learning P ar t 1: Fr om single neur ons to m ulti-la y er per ceptr ons Dr . Richard E. T ur ner ( ret26@cam.ac.uk ) Engineer ing T r ipos P ar t IB P aper 8: Inf or mation Engineer ing Example per ception pr ob lems f or a self-driving car road paveme nt building car 1 cyclist 1 cyclist 2 car 2 t rees t ra f f ic light s pe dest r ian 1 t ra f f ic light s int ent lane cyclist 3 signals green V er y har d to hand-design systems to do this reliab l y fairl y easy to collect lots of labelled data = ) learn systems fr om data instead Object recognition • F ocus on objection recognition : predict objects present in an image • A canonical classiﬁcation prob lem: approaches to object recognition can be gener alised to more comple x tasks • All state-of-the-ar t approaches use neural netw orks • Outline of lectures: – single neurons and logistic classiﬁcation (2 lectures) – neur al netw or ks and m ulti-la y er perceptrons (1 lecture) – con v olutional neur al netw or ks (2 lectures) – w or k ed e xamples integ r ated through out (no speciﬁc e xamples class) Current state-of-the-ar t https://www.c larifai.com/demo A single neur on −5 0 5 0 0. 5 1 out p ut of neuron act ivit y of neuron inp ut s t o neuron biological neuron A rt if icial neuron comp ut es a non-linear f unct ion of inp ut s: A single neur on −5 0 5 0 0. 5 1 out p ut of neuron act ivit y of neuron inp ut s t o neuron biological neuron A rt if icial neuron comp ut es a non-linear f unct ion of inp ut s: Input-output function of a single neur on − 5 0 5 − 5 0 5 0 0.2 0.4 0.6 0.8 1 z 2 w = [0,1] z 1x z 1z2 − 5 0 5 − 5 0 5 x ( z 1 ,z 2 )= 1 1 +e x p ( \u0000 w 1 z 1 \u0000 w 2 z 2 ) Input-output function of a single neur on − 5 0 5 − 5 0 5 0 0.2 0.4 0.6 0.8 1 z 2 w = [0.8,0.6] z 1x z 1z2 − 5 0 5 − 5 0 5 x ( z 1 ,z 2 )= 1 1 +e x p ( \u0000 w 1 z 1 \u0000 w 2 z 2 ) Input-output function of a single neur on − 5 0 5 − 5 0 5 0 0.2 0.4 0.6 0.8 1 z 2 w = [0,5] z 1x z 1z2 − 5 0 5 − 5 0 5 x ( z 1 ,z 2 )= 1 1 +e x p ( \u0000 w 1 z 1 \u0000 w 2 z 2 ) Input-output function of a single neur on \u0000 5 0 5 \u0000 5 0 5 0 0.2 0.4 0.6 0.8 1 z 2 w = [0,1] z 1x z 1z2 \u0000 5 0 5 \u0000 5 0 5 con t o ur s set s dir e ct i on o f bo un da r y set s st e epn ess of b ou nd ar y x ( z 1 ,z 2 )= 1 1 +e x p ( \u0000 w 1 z 1 \u0000 w 2 z 2 ) W eight space of a single neur on \u0000 5 0 5 \u0000 5 0 5 0 0.5 1 \u0000 5 0 5 \u0000 5 0 5 0 0.5 1 \u0000 5 0 5 \u0000 5 0 5 0 0.5 1 \u0000 5 0 5 \u0000 5 0 5 0 0.5 1 \u0000 5 0 5 \u0000 5 0 5 0 0.5 1 \u0000 5 0 5 \u0000 5 0 5 0 0.5 1 -2 0 2 -2 0 24 W 1 W 2 \u0000 5 0 5 \u0000 5 0 5 0 0.5 1 \u0000 5 0 5 \u0000 5 0 5 0 0.5 1 \u0000 5 0 5 \u0000 5 0 5 0 0.5 1 \u0000 5 0 5 \u0000 5 0 5 0 0.5 1 \u0000 5 0 5 \u0000 5 0 5 0 0.5 1 \u0000 5 0 5 \u0000 5 0 5 0 0.5 1 W = [ 2 , 2 ] z z 1 2 x T raining a single neur on 0 1 0 0 class class t ra ining da t a o b j ecti ve fu n c ti o n : inp ut s class lab els neuron out p ut s d esi red resu l t o f trai n i n g : f or neuron out p ut s f or 0 1 0 1 if if consider single t erm in sum quant if ies qualit y of in t erms of \" f it \" t o t raining d at a T raining a single neur on 0 1 0 0 class class t ra ining da t a o b j ecti ve fu n c ti o n : relat ive ent ropy bet ween and surprise when observing inp ut s class lab els neuron out p ut s d esi red resu l t o f trai n i n g : f or small when neu ron out pu t ' close' t o t raining d at a neuron out p ut s f or 0 1 0 1 if if consider single t erm in sum T raining a single neur on: a pr ob lem 0 1 0 0 t ra ining da t a o b j ecti ve fu n c ti o n : T h er e i s n o a n a l y ti c s o l u ti o n to th i s o p ti mi s a ti o n p r o b l em : w h a t s h o u l d w e d o ? inp ut s class lab els choose wei ght s t hat minimise net work' s sur p rise ab out t ra ining da t a Goal of training objective parameters * * T raining using gradient descent objective parameters weight at iteration i (initialised randomly at first iteration, i=1) gradient points away from (local) minimum take a step in opposite direction to gradient learning rate T raining using gradient descent objective parameters 2 T raining using gradient descent objective parameters 3 2 2 T raining using gradient descent objective parameters 3 3 2 T raining using gradient descent objective parameters 3 run until convergence criteria reached or time runs out 2 What happens if the learning rate ⌘ is set too lar g e? objective parameters set too large What happens if the learning rate ⌘ is set too small? objective parameters set too small T raining a single neur on: a solution 0 1 0 0 t ra ining da t a o b j ecti ve fu n c ti o n : T h er e i s n o a n a l y ti c s o l u ti o n to th i s o p ti mi s a ti o n p r o b l em : w h a t s h o u l d w e d o ? An s w er : u s e g ra d i e n t d es c en t inp ut s class lab els choose wei ght s t hat minimise net work' s sur p rise ab out t ra ining da t a T raining a single neur on 0 1 0 0 t ra ining da t a o b j ecti ve fu n c ti o n : inp ut s class lab els predict ion error choose wei ght s t hat minimise net work' s sur p rise ab out t ra ining da t a it erat ively st ep down t he object ive (gr ad ient p oint s up hill) f eat ure T raining a single neur on − 5 0 5 − 5 0 5 0 0.2 0.4 0.6 0.8 1 z 2 w = [0, − 1] z 1x − 5 0 5 − 5 0 5 z 1z2 0 5 10 15 20 10 0 iterationobjective T raining a single neur on − 5 0 5 − 5 0 5 0 0.2 0.4 0.6 0.8 1 z 2 w = [2.2,2.3] z 1x − 5 0 5 − 5 0 5 z 1z2 0 5 10 15 20 10 0 iterationobjective T raining a single neur on − 5 0 5 − 5 0 5 0 0.2 0.4 0.6 0.8 1 z 2 w = [9.7,25.3] z 1x − 5 0 5 − 5 0 5 z 1z2 0 10 20 30 40 50 10 − 5 10 0 iterationobjective Over -ﬁtting and weight deca y 0 1 0 0 t ra ining da t a o b j ecti ve fu n c ti o n : \" reg ulariser\" : penalise m ag nit ude of t he weight s a nd d iscourag es t he net work f r om using ext reme weight s weight deca y - shrinks weight s t oward s zero inp ut s class lab els T raining a single neur on − 5 0 5 − 5 0 5 z 1z2 w reg = [0, − 1] − 5 0 5 − 5 0 5 z 1z2 w = [0, − 1] 0 10 20 30 40 50 10 0 iterationobjective original regularised T raining a single neur on − 5 0 5 − 5 0 5 z 1z2 w reg = [1,1.1] − 5 0 5 − 5 0 5 z 1z2 w = [2.5,4] 0 10 20 30 40 50 10 0 iterationobjective original regularised Pr obabilistic interpretation of the single neur on 0 1 0 0 I nt rep ret t he out put of t he net work as a probabilit y: \" choose set t ing of w t ha t makes ob served d at a most p roba ble\" F it t ing met hod 1: maximum-likel ihood learning \" p r o b a b i l i ty o f l a b el s g i v en th e i n p u ts a n d p a r a me ters \" o r \" l i k el i h o o d o f th e p a ra me ters \" mo n o to n i c i ty o f l o g a ri th m p r es e rv es l o c a ti o n o f o p ti mu m c o m p u ta ti o n a l l y a n d ma th ema ti c a l l y s i mp l er to w o rk w i th l o g - l i k el i h o o d s Pr obabilistic interpretation of the single neur on \" choose set t ing of w t ha t is most probab le given t he da t a and p rior knowledge\" 0 1 0 0 I nt repret t he out put of t he net work as a probabilit y: F it t ing met hod 2: maximum a po st eriori (MA P ) po st erior d ist ribut ion (what we know af t er seeing d at a ) prior dist ribut ion (what we knew bef ore seeing da t a) likelih ood of t he para met ers (what t he da t a t ell us) B ayes' rule wit h , and normalising const ant does not depend on w use a zero mean, ind ep endent G aussia n p rior Pr obabilistic interpretation of the single neur on \" choose set t ing of w t ha t is most probab le given t he da t a and p rior knowledge\" 0 1 0 0 I nt repret t he out put of t he net work as a probabilit y: F it t ing met hod 2: maximum a po st eriori (MA P ) po st erior d ist ribut ion (what we know af t er seeing d at a ) prior dist ribut ion (what we knew bef ore seeing da t a) likelih ood of t he para met ers (what t he da t a t ell us) B ayes' rule wit h , and normalising const ant use a zero mean, ind ep endent G aussia n p rior Pr obabilistic interpretation of the single neur on \" choose set t ing of w t ha t is most probab le given t he da t a and p rior knowledge\" 0 1 0 0 I nt repret t he out put of t he net work as a probabilit y: F it t ing met hod 2: maximum a po st eriori (MA P ) po st erior d ist ribut ion (what we know af t er seeing d at a ) prior dist ribut ion (what we knew bef ore seeing da t a) likelih ood of t he para met ers (what t he da t a t ell us) B ayes' rule wit h , and normalising const ant subst it ut e in log prior use a zero mean, ind ep endent G aussia n p rior Pr obabilistic interpretation of the single neur on \" choose set t ing of w t ha t is most probab le given t he da t a and p rior knowledge\" 0 1 0 0 I nt repret t he out put of t he net work as a probabilit y: F it t ing met hod 2: maximum a po st eriori (MA P ) does not depend on w po st erior d ist ribut ion (what we know af t er seeing d at a ) prior dist ribut ion (what we knew bef ore seeing da t a) likelih ood of t he para met ers (what t he da t a t ell us) B ayes' rule wit h , and normalising const ant use a zero mean, ind ep endent G aussia n p rior Pr obabilistic interpretation of the single neur on \" choose set t ing of w t ha t is most probab le given t he da t a and p rior knowledge\" 0 1 0 0 I nt repret t he out put of t he net work as a probabilit y: F it t ing met hod 2: maximum a po st eriori (MA P ) po st erior d ist ribut ion (what we know af t er seeing d at a ) prior dist ribut ion (what we knew bef ore seeing da t a) likelih ood of t he para met ers (what t he da t a t ell us) B ayes' rule wit h , and normalising const ant use a zero mean, ind ep endent G aussia n p rior Pr obabilistic interpretation of the single neur on \" choose set t ing of w t ha t is most probab le given t he da t a and p rior knowledge\" 0 1 0 0 I nt repret t he out put of t he net work as a probabilit y: F it t ing met hod 2: maximum a po st eriori (MA P ) po st erior d ist ribut ion (what we know af t er seeing d at a ) prior dist ribut ion (what we knew bef ore seeing da t a) likelih ood of t he para met ers (what t he da t a t ell us) B ayes' rule wit h , and normalising const ant use a zero mean, ind ep endent G aussia n p rior where Mac hine learning: rebranding statistics? Machine Lea rning t er mino logy S t a t ist ical t erminology single neuron f or classif icat ion logist ic regr ession or logist ic classif icat ion relat ive ent ropy (dat a f it t er m) log-likelihood f unct ion regular isat ion (weight deca y t erm) minim ise relat ive ent ropy + regular iser maximum a po st eriori f it t ing maximum li kelihoo d lea rningminim ise of relat ive ent ropy regular isat ion (log-prior) Single hid den la y er neural netw orks 1 0 out p ut of net work act ivit y of out put neuron hidden neuron out pu t s hidden neuron act ivit ies inp ut s dimensionalit y( ) = dimensionalit y( ) = out p ut weight s inp ut weight s Sampling random neural netw ork c lassiﬁer s − 5 0 5 − 5 0 5 0 0.2 0.4 0.6 0.8 1 z 2 z 1x z 1z2 − 5 0 5 − 5 0 5 Sampling random neural netw ork c lassiﬁer s − 5 0 5 − 5 0 5 0 0.2 0.4 0.6 0.8 1 z 2 z 1x z 1z2 − 5 0 5 − 5 0 5 Bac kpr opa gation o b j ecti ve fu n c ti o n : regular iser discoura ges ext r eme weight s likelihood same as bef ore out put of net work act ivit y of out put neuro n hidden neuron out put s hidden neuron act ivit ies inp ut s chain rule = ba ckward pass t hrough net work = \" b ackprop ag at ion\" T raining a neural netw ork with a single hid den la y er \u0000 5 0 5 \u0000 5 0 5 0 0.2 0.4 0.6 0.8 1 z 2 z 1x \u0000 5 0 5 \u0000 5 0 5 z 1z2 T raining a neural netw ork with a single hid den la y er − 5 0 5 − 5 0 5 0 0.2 0.4 0.6 0.8 1 z 2 z 1x − 5 0 5 − 5 0 5 z 1z2 Multi-la y er per ceptr ons: hierar c hical models with man y hid den la y er s in pu t s la yer ou t p ut hi dd en layer A P otted Histor y of Neural Netw orks 1980s Fir st wa ve of neural netw ork de velopment – m ulti-la y er perceptrons , con v olutional neur al netw or ks , g r adien t based optimisation etc. – limited tr aining success caused prog ress to stall. 2010s Second wa ve of neural netw ork de velopment due to 1. more data (e .g. imageNet) 2. more compute (e .g. GPUs) 3. technical adv ances (e .g. initialisation, neuron non-linear ities , model architecture) 2019 s Bespoke software f or designing & training neural netw orks – automatic diff erentiation, simple deplo yment to GPUs etc. – e xamples include PyT orch and T ensorFlo w – softw are 2.0: neur al netw or k design = cur ating a prog r amme with each la y er being a step , no w lear n precise f or m from data Pr ogression of P erf ormance on Object Reconition Summar y of P ar t I • Super vised ar tiﬁcial neural netw orks ﬁt a non-linear function x that maps from input f eatures ( z ) to output targets ( y ), lik e a f ancy f or m of cur v e ﬁtting. • Netw or ks with se v er al hidden la y ers are kno wn as m ulti-la y er per ceptr ons and can be ﬁt b y optimising an objective function by gradient descent . • The objectiv e function compr ises a data ﬁt term G ( w ) that ensures the output of the netw or k matches the tr aining data and a regularisation term E ( w ) to stop o ver -ﬁtting . These ter ms can be related to the likelihood function and prior of the w eights from the ﬁeld of statistics . • The g r adient computation requires the chain r ule and is kno wn as bac kpr opa gation . • Smar t initialisation is required to stop lear ning f alling into poor local optima . Demo: http://yann.lecun.com/exdb/lenet/","libVersion":"0.2.4","langs":""}