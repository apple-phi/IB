{"path":"P7 - Maths/_handouts/Linalg 5.pdf","text":"1 IB Paper 7: Linear Algebra Handout 5 5. Properties of the Fundamental Subspaces of A 5.1 Bases for the Four Spaces for AI and AII In order to discuss the properties of the subspaces further it is useful to complete the process of finding bases for the two examples examined in section 3.7 and 4.3 CASE I A = 12 1 3 26 3 12 12 2 8 ⎡⎤ ⎢⎥ ⎢⎥ ⎢⎥ ⎢⎥−⎣⎦ CASE II A = 12 1 3 26 3 12 12 1 9 ⎡ ⎤ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥−⎣ ⎦ The story so far is LU = 10 0 21 0 12 1 ⎡⎤ ⎢⎥ ⎢⎥ ⎢⎥ ⎢⎥−⎣⎦ 12 1 3 02 1 6 00 1 1 ⎡⎤ ⎢⎥ ⎢⎥ ⎢⎥ ⎢⎥−⎣⎦ LU = 10 0 21 0 12 1 ⎡ ⎤ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥−⎣ ⎦ 12 1 3 02 16 000 0 ⎡⎤ ⎢⎥ ⎢⎥ ⎢⎥ ⎢⎥⎣⎦ Column Space (the columns of L used, i.e. corresponding to non-zero rows of U) 10 0 2, 1 and 0 12 1 ⎡⎤ ⎡ ⎤ ⎡ ⎤ ⎢⎥ ⎢ ⎥ ⎢ ⎥ ⎢⎥ ⎢ ⎥ ⎢ ⎥ ⎢⎥ ⎢ ⎥ ⎢ ⎥ ⎢⎥ ⎢ ⎥ ⎢ ⎥−⎣⎦ ⎣ ⎦ ⎣ ⎦ 10 2 and 1 12 ⎡ ⎤⎡ ⎤ ⎢ ⎥⎢ ⎥ ⎢ ⎥⎢ ⎥ ⎢ ⎥⎢ ⎥ ⎢ ⎥⎢ ⎥−⎣ ⎦⎣ ⎦ Null Space (= Null space of U) (Set the free variables to 1 in turn and solve Ux = 0) 3 7/ 2 1 1 ⎡⎤ ⎢⎥−⎢⎥ ⎢⎥ ⎢⎥ ⎣⎦ 30 31 2 and 01 10 ⎡ ⎤⎡ ⎤ ⎢ ⎥⎢ ⎥−−⎢ ⎥⎢ ⎥ ⎢ ⎥⎢ ⎥ ⎢ ⎥⎢ ⎥ ⎣ ⎦⎣ ⎦ Row Space All non-zero rows of U 10 0 22 0 , and 11 1 36 1 ⎡⎤ ⎡⎤ ⎡ ⎤ ⎢⎥ ⎢⎥ ⎢ ⎥ ⎢⎥ ⎢⎥ ⎢ ⎥ ⎢⎥ ⎢⎥ ⎢ ⎥ ⎢⎥ ⎢⎥ ⎢ ⎥ −⎣⎦ ⎣⎦ ⎣ ⎦ 10 22 and 11 36 ⎡⎤⎡ ⎤ ⎢⎥⎢ ⎥ ⎢⎥⎢ ⎥ ⎢⎥⎢ ⎥ ⎢⎥⎢ ⎥ ⎣⎦⎣ ⎦ Left Null Space 0 5 2 1 ⎡ ⎤ ⎢ ⎥−⎢ ⎥ ⎢ ⎥⎣ ⎦ see later 2 5.2 A few useful properties of the Fundamental Subspaces (a) The dimension of row-space is thus equal to the number of non-zero rows of U which is equal to the number of basic variables (those with pivots). So Dimension of row space = r = rank(A) (b) The dimension of the null space of A is equal to the number of free variables (i.e. the number of variables without pivots). Thus Dimension of null space = n − r . (c) Suppose n is in the null space of A, this mean that 0 = A n = 1 2 ... ... ... m a a a ←→⎡⎤ ⎢⎥←→⎢⎥ ⎢⎥ ⎢⎥ ←→⎣⎦ % % % n ⎡⎤↑ ⎢⎥ ⎢⎥ ⎢⎥↓⎣⎦ = 1 2 ... m an an an ⎡ ⎤ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎣ ⎦ . . . % % % This means that a vector n is in null-space if, and only if, it is orthogonal to every row of A and hence orthogonal to every vector in row space. i.e. Null space and Row Space are orthogonal (d) Everything we have said about A, applies to AT. Thus the dimension of left null-space = m − r . Every vector in Column Space is orthogonal to every vector in Left Null Space. Row space and Null Space are said to be orthogonal complements in Rn, and Column Space and Left Null space are orthogonal complements in Rm. Which means that they are orthogonal and between them they have a total number of dimensions which is equal n and m respectively. (i.e. there is “nowhere else to go” in Rn and Rm. 5.3 Orthogonal Complementary Subspaces Let us step back to 3D for a moment and imagine we have a vector sub-space consisting of a plane through the origin spanned by the vectors u and v. i.e. the plane is Sub-space 1 x uvα β=+ This is a two-dimensional subspace of R3 . The line through the origin that is perpendicular to this plane is a one-dimensional subspace of R3 Sub-space 2 x tλ= These sub-spaces are orthogonal complements. 0 line through 0 3 They have the properties that: (a) Any vector can be written as a combination of a vector in each of the sub-spaces 12bb b=+ 1bu vα β=+ 2btλ= (b) Every vector in one space is perpendicular to every vector in the other. 12 0bb =. For 4D, we would have Sub-space 1 x uvα β=+ This is a two-dimensional subspace of R4 . There are now two dimensions orthogonal to this and so the orthogonal complement of this subspace would also be two-dimensional. Sub-space 2 x tsλ μ=+ Again, every vector in R4 can be written 12bb b=+ with 12 0bb =. In R4 we could also have complementary subspaces consisting of “lines” and “volumes” Sub-space 1 x uv wα βγ=+ + Sub-space 2 x tλ= For orthogonal complements, the sum of dimensions of the two subspaces must always equal the total for the space. 0 R4 2 dimensions 2 dimensions b b1 b2 orthogonal 0 line through 0 v u 3 dimensions t w 4 5.4 The Big Picture Row space and Null space represent all of Rn , in the sense that any x can be written row nullxx x=+ and 0row nullxx =. Column space and Left Null space similarly carve up Rm col leftbb b=+ 0col leftbb =. Notes 1) If a vector b has a non-zero leftb then A x = b has no solution. The various compatibility conditions necessary for there to be a of solution of A x = b can also be stated as b being orthogonal to each of the vectors in a basis of Left Null Space. 2) If we restrict the vectors that A operates on to Row Space, then it is clear that A maps an r- dimensional space onto Column Space (which is also r-dimensional). The mapping is, therefore, reversible. Row Space Null Space orthogonal Column space Left Null Space orthogonal 5 5.5 Finding Left-Null space The fact that Left-Null Space is perpendicular to Column Space is the clue to the easiest way to find it. It should be clear for CASE I (m = 3, n = 4), for example, that because r = 3 the dimension of left null space must be 3 − 3 = 0 (i.e. null space is just 0). For CASE II, AII = 12 1 3 26 3 12 12 1 9 ⎡⎤ ⎢⎥ ⎢⎥ ⎢⎥ ⎢⎥−⎣⎦ = 10 0 21 0 12 1 ⎡ ⎤ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥−⎣ ⎦ 12 1 3 02 16 000 0 ⎡ ⎤ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥⎣ ⎦ Any vector in the left null space is orthogonal to every vector in column space and the basis of column space is the first two columns of L. So if n is in null-space 1T RED 2 l b l ←→⎡⎤ = ⎢⎥←→⎣⎦ L b ⎡⎤↑ ⎢⎥ ⎢⎥ ⎢⎥↓⎣⎦ = 1 2 lb lb ⎡ ⎤ ⎢ ⎥ ⎣ ⎦ . . ⇒ 1 2 3 12 1 01 2 0 0 b b b ⎡⎤ − ⎢⎥⎡⎤ ⎢⎥⎢⎥ ⎢⎥⎢⎥⎣⎦ ⎢⎥⎣ ⎡⎤ = ⎢⎥ ⎢⎣⎦ ⎦ ⎥ b3 is a free variable. If we take it equal to 1, then b3 = 1 , b2 = −2 and b1 = b3 − 2 b2 = 5, giving a basis of the left null space 5 2 1 ⎡⎤ ⎢⎥ ⎢⎥− ⎢⎥ ⎢⎥⎣⎦ If there is more than one “free value”, treat them just as we did the free variables in the general solution of Ax = b, i.e. take then as unity in turn with the rest 0. 6. Final Bits and Pieces 6.1 Algorithmic Complexity of LU We saw in section 3.5 that LU decomposition with partial pivoting is a robust algorithm. The other quality we would want from a good general purpose algorithm is that it should be quick for large values of m and n. How many operations does it take to: Complete the LU factorisation ? Complete the solution of A x = b? Different computers take different amounts of time to perform different operations (for example division used to be very slow relative to addition or multiplication) and there is some ambiguity as to what constitutes an \"operation\". We will estimate the number of \"operations\" by considering an n × n (i.e. a square) case. We will ignore assignment statements and count each addition, subtraction, multiplication and division as one operation. The results will be rough, but will give an order of magnitude estimate of how much cpu time is involved. 6 We achieve zeros in the first column of U by subtracting a multiple of the first row from each of the succeeding rows in turn. We start with the second row and put 21 21 11 u l u = 22 21 1*jj juu l u= − for j = 1 to n which is 1 + 2n ops (1 for multiplier) There are n − 1 rows to deal with leading to ( )( )12 1nn− + ops * = (possibly) non-zero element To achieve zeros in the second column of U, we repeat the process but only deal with the third and subsequent row and don't have to touch the first column (which is all zeros). 32 32 22 u l u = 33 32 2*jj juu l u= − for j = 2 to n which is ()12 1n+ ×− ops There are n − 2 rows to deal with leading to ()( )22 1nn−− ops To achieve zeros in the k'th column 1, 1, kk kk kk u l u + + = 1, 1, 1, *kj k j k k kjuu l u++ += − for j = k to n which is ( )121 +−+ kn ops There are n − k rows to deal with leading to ( )( )322 +−− knkn operations. The total number of operations to complete the LU factorisation is, therefore, (the process is complete when 1−n columns have been dealt with) () ()[]∑ − = −+− 1 1 2 32 n k knkn = []∑ − = + 1 1 2 32 n i ii = ( )( ) ( )12 1 1 23 62 nn n n n−− − + ≈ 3 3 2 n The forward and back substitution (finding Lc = b and solving Ux = c ) is considerably quicker. It takes one operation for the first equation, three for the second, 5 for the third, something like: b/az = ()c/z*edy += ( )f/z*iy*hgx ++= which makes in total () 1 21 n k k = −∑ ≈ 2n ** * * 0* * * 0* * * 0* * * ⎡⎤ ⎢⎥ ⎢⎥ ⎢⎥ ⎢⎥ ⎣⎦ ** * * 0* * * 00 * * 00 * * ⎡⎤ ⎢⎥ ⎢⎥ ⎢⎥ ⎢⎥ ⎣⎦ 7 For large n, the LU factorisation requires ≈ 3 3 2 n operations. Once the LU factorisation has been completed, it requires ≈ 2 2n operations to complete the solution of A x = b These numbers are remarkably small. For comparison let us compare simply multiplying two nn × matrices together AX = B. To form each element of B, we take ijb = 1 2 12 j j ii in nj x x aa a x ⎡ ⎤⎡⎤ ⎢ ⎥⎢⎥ ⎢ ⎥⎢⎥ ⎢ ⎥⎢⎥ ⎢ ⎥⎢⎥ ⎢ ⎥⎢⎥ ⎢ ⎥⎢⎥⎣⎦⎢ ⎥⎣ ⎦ ... ... = 11 2 2ij i j in njax a x a x+ ++... n2 elements, each element is a dot product, dot product = n multiplications + n −1 additions By this way of reckoning operations, then, it requires ()1 2 −+ nnn ≈ 32n to multiply two nn × matrices together and ≈ 22n to multiply an nn × matrix and a vector. 6.2 Some other methods involving manipulations of rows of matrices. We showed in handout 2, that the product A = B C where A is an m × n matrix, B is an m × k matrix, C is an k × n matrix, is equivalent to [ ] [ ] [] 11 12 1 21 22 2 12 1 2 12 nn n kk kn k cc c c c c aa a b b cc c b ⎡⎤ ⎡ ⎤ ⎡ ⎤↑↑ ↑ ↑ ↑ ⎢⎥ ⎢ ⎥ ⎢ ⎥ =+ +⎢⎥ ⎢ ⎥ ⎢ ⎥ ⎢⎥ ⎢ ⎥ ⎢ ⎥↓↓ ↓ ↓ ↓⎣⎦ ⎣ ⎦ ⎣ ⎦ ⎡⎤↑ ⎢⎥ + ⎢⎥ ⎢⎥↓⎣⎦ ... ... ... ... ... ... ... Let us assume that, as is the case of U when we are doing LU decomposition, that B is square and C is the same shape as A. i.e. k = m 8 [ ] [ ] [] 11 12 1 21 22 2 12 1 2 12 nn n mm mn m cc c c c c aa a b b cc c b ⎡⎤ ⎡ ⎤ ⎡ ⎤↑↑ ↑ ↑ ↑ ⎢⎥ ⎢ ⎥ ⎢ ⎥ =+ +⎢⎥ ⎢ ⎥ ⎢ ⎥ ⎢⎥ ⎢ ⎥ ⎢ ⎥↓↓ ↓ ↓ ↓⎣⎦ ⎣ ⎦ ⎣ ⎦ ⎡⎤↑ ⎢⎥ + ⎢⎥ ⎢⎥↓⎣⎦ ... ... ... ... ... ... ... Suppose we now add an extra column at the end of the row C to get a larger matrix . Cext = 11 12 1 1 21 22 2 2 12 ... ... ... ... ... ... ... ... n n mm mn m cc c d cc c d cc c d ⎡⎤ ⎢⎥ ⎢⎥ ⎢⎥ ⎢⎥ ⎣⎦ where the extra column is represented by d. This extends the matrix A by a column, which we denote as e. Aext = 11 12 1 1 21 22 2 2 12 ... ... ... ... ... ... ... ... n n mm mn m aa a e aa a e aa a e ⎡⎤ ⎢⎥ ⎢⎥ ⎢⎥ ⎢⎥ ⎣⎦ The original columns of the matrix A are unaltered by this and is given by [ ] [ ] [ ]12 12 ... m m dd d eb b b ⎡⎤ ⎡ ⎤ ⎡ ⎤ ⎡ ⎤↑↑ ↑ ↑ ⎢⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥ =+ + +⎢⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥ ⎢⎥ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥↓↓ ↓ ↓⎣⎦ ⎣ ⎦ ⎣ ⎦ ⎣ ⎦ and we can see that the column vector d is simply multiplied by the matrix B to get e. i.e. [ A e ] = B [ C d ] = [ BC Bd ] This will in fact work with matrices D and E provided they have the same number of rows as A, so that the combined matrix makes sense. [ A E ] = B [ C D ] = [ BC BD ] This technique is sometimes used by programs using LU decomposition to solve A x = b to shortcut the solution of Lc = b, we stick b in as an extra final column to A and then do our LU factorisation as normal to get [ A b ] = L [ U c ] Given that L [ U c ] = [ LU Lc ] it is clear that LU on the extended matrix will furnish c where Lc = b. If this is part of a process that is being repeated many times, i.e. we seek to solve A x = b for a number of b’s, we simply collect them into a matrix B and add this as extra columns to A 9 [ A B ] → by LU to L [ U C ] = [ LU LC ] and the extended U matrix, [ U C ], contains all of the c’s as columns of C. We discussed earlier that LU decomposition ends up with the rows of A as linear combinations of the rows of U, where the coefficients accumulate in the matrix L. We had a particular pattern in mind for U when we started the process. In fact, any way of manipulating U by taking multiples of one row and adding them to others would similarly accumulate the coefficients in L, although U would not then be upper echelon and L would not be lower triangle. One final note, remembering that a row of U contributes to a row of A through the product TT T 11 2 2 3 3lu l u l u=+ + +A ...%% % we could also divide a row of U by a factor α provided we scale multiply the corresponding column of L by 1/α . Consider the following manipulation Take a 3 × 3 matrix A and extend it by adding the 3 × 3 identity matrix I. We will take various linear combinations of each row of the extended matrix away from each other and do the odd row scaling to make the first three columns of the extended U to be the identity matrix. [ A I ] = L [ I D ] We are not interested in the accumulated coefficients stored in the matrix L, so will not keep track of it, but it is conceptually there. e.g. 11 2 1 1 2 1 0 0 11 1 1 1 1 0 1 0 12 3 1 2 3 0 0 1 −−⎡⎤ ⎡ ⎤ ⎢⎥ ⎢ ⎥=− − → − −⎢⎥ ⎢ ⎥ ⎢⎥ ⎢ ⎥⎣⎦ ⎣ ⎦ A Subtract 1 × top row from the second and the third 11 2 1 0 0 02 1 1 1 0 01 5 1 0 1 −⎡ ⎤ ⎢ ⎥−−⎢ ⎥ ⎢ ⎥−⎣ ⎦ Take a factor –2 out of the second row 11 2 1 0 0 11 1 01 0 22 2 01 5 1 0 1 −⎡ ⎤ ⎢ ⎥ ⎢ ⎥−− ⎢ ⎥ ⎢ ⎥−⎣ ⎦ Subtract 1 × middle row from the top and the third 31 1 10 0 22 2 11 1 01 0 22 2 11 3 1 00 1 22 2 ⎡ ⎤ −⎢ ⎥ ⎢ ⎥ ⎢ ⎥−− ⎢ ⎥ ⎢ ⎥ ⎢ ⎥− ⎢ ⎥⎣ ⎦ 10 Multiply the third row by a factor 2 11 31 1 10 0 22 2 11 1 01 0 22 2 31 2 00 1 11 11 11 ⎡ ⎤ −⎢ ⎥ ⎢ ⎥ ⎢ ⎥−− ⎢ ⎥ ⎢ ⎥ ⎢ ⎥− ⎢ ⎥⎣ ⎦ Add 1 2 of the bottom row to the middle and 3 2 of the bottom row to the top 33 1 1 3 1 23 100 11 2 2 11 2 2 11 2 31 1 1 1 1 21 01 0 11 2 2 11 2 2 11 2 31 2 001 11 11 11 ⎡⎤ −+ +⎢⎥ ⎢⎥ ⎢⎥−+ − ⎢⎥ ⎢⎥ ⎢⎥− ⎢⎥⎣⎦ = 17 3 10 0 11 11 11 45 1 01 0 11 11 11 31 2 00 1 11 11 11 ⎡ ⎤ ⎢ ⎥ ⎢ ⎥ ⎢ ⎥− ⎢ ⎥ ⎢ ⎥ ⎢ ⎥− ⎢ ⎥⎣ ⎦ = [ I D ] It should be clear that D = A −1. If we had kept track of the matrix L, it would have become A. Key Points from Lecture The four fundamental subspaces associated with A, partition Rm and Rn as shown. Column Space Left Null Space Row Space Null Space dimension n − r dimension rdimension r dimension m − r 0 0 A A Rn Rm Can’t get here with A x y here ⇒ y ⊥ every column of A Every vector here ⊥ every vector here 11 Summary Rank The rank, r, of a matrix is the number of independent rows, or columns. Fundamental Subspaces of an m × n matrix A The column space is the space spanned by the columns. It has dimension equal to the rank, r, and is a subspace of mR . The nullspace is the space spanned by the solutions x of the equation Ax = 0. The nullspace has dimension n − r and is a subspace of nR . The row space is the space spanned by the rows of A. It has dimension equal to r and is a subspace of nR . The left-nullspace is the space spanned by the solutions y of the equation yt A = 0. It has dimension m − r, and is a subspace of mR . The nullspace is the orthogonal complement of the row space in nR . The left-nullspace is the orthogonal complement of the column space in mR . For bxA = to have a solution, b must lie in the column space, i.e. yt b = 0 for any y such that At y = 0","libVersion":"0.2.4","langs":""}